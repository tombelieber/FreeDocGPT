# ğŸ“š FreeDocGPTï¼ˆå…è²»æ–‡ä»¶GPTï¼‰

Language Quick Start Â· èªè¨€ Â· è¯­è¨€ Â· Idiomas Â· è¨€èª
- English: see [Quick Start (EN)](#quick-start-en)
- ç¹é«”ä¸­æ–‡: åƒè¦‹ [å¿«é€Ÿé–‹å§‹ï¼ˆç¹ä¸­ï¼‰](#quick-start-zh-hant)
- ç®€ä½“ä¸­æ–‡: å‚è§ [å¿«é€Ÿå¼€å§‹ï¼ˆç®€ä¸­ï¼‰](#quick-start-zh-hans)
- EspaÃ±ol: ver [Inicio RÃ¡pido (ES)](#quick-start-es)
- æ—¥æœ¬èª: [ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ (JA)](#quick-start-ja)

Your free, local document AI assistant â€” read, search, ask, and learn from multiple files without cloud costs or API keys.

> é›¶æˆæœ¬ã€æœ¬åœ°é‹è¡Œã€ç§å¯†å®‰å…¨ã€‚æŠŠæª”æ¡ˆä¸Ÿé€² `documents/`ï¼Œå°±èƒ½é–±è®€ã€æœå°‹ã€å•ç­”ã€æ•´ç†ã€å­¸ç¿’ã€‚

With local models like gpt-oss:20b and Embedding Gemma available in Ollama, you can run this AI document helper fully on your Mac â€” no cloud, no signâ€‘ups. Just drop files and ask.

## ğŸ’¬ Community & Support

Join our Discord community for support, discussions, and updates: [https://discord.gg/usRtaeY8](https://discord.gg/usRtaeY8)

- ğŸ¤ Get help from the community
- ğŸ› Report issues and bugs
- ğŸ’¡ Share feature ideas and feedback  
- ğŸ“¢ Stay updated on new releases

â€”

Table of Contents
- [Why This Project & Why Now](#-why-this-project--why-now)
- [For Everyone (No Tech Needed)](#-for-everyone-no-tech-needed)
- [For Technical Users](#-for-technical-users)
- [Quick Start (EN)](#quick-start-en)
- [Features](#-features)
- [Configuration (.env)](#-configuration--env)
- [Tips](#tips)
- [Languages](#languages)
- [Testing (optional)](#testing-optional)
- [Privacy & Requirements](#privacy--requirements)

## ğŸŒ Why This Project & Why Now

Openâ€‘source model releases make private, onâ€‘yourâ€‘Mac AI document help possible for everyone â€” even if you're not technical:

- `gpt-oss:20b` can answer questions clearly and fluently.
- Embedding Gemma helps the app quickly "remember" what's in your files.
- With Ollama, these run on your computer â€” no cloud accounts, no fees, and your files never leave your device.

Why we built FreeDocGPT:
- So you don't miss this moment just because it used to be "too technical".
- A friendly app: drop files, press "Index", then ask questions in plain language.
- Sensible defaults baked in; you don't need to know how it works inside.

In short: Modern local models make trustworthy, private document help possible at home. FreeDocGPT puts it one click away.

## ğŸ‘‹ For Everyone (No Tech Needed)

- Put your files in the `documents/` folder (PDF, Word, Markdown, etc.).
- Click â€œğŸ”„ Index New Documentsâ€ in the sidebar.
- Ask questions like â€œSummarize this contractâ€ or â€œWhere is the API auth section?â€.
- See the original sources used for the answer.
- Everything stays on your Mac; after the first run, it can work offline.

Tip: If you see unfamiliar terms, ignore them â€” the default settings already give good results.

## ğŸš€ Quick Start (EN)
<a id="quick-start-en"></a>

### 1) Install prerequisites (macOS)
```bash
brew install ollama           # Local LLM runtime
ollama serve &                # Start Ollama
ollama pull gpt-oss:20b      # QA / chat model
ollama pull embeddinggemma:300m  # Embedding model for search
ollama pull llava:7b         # (Optional) Vision model for image Q&A
```

### 2) Setup Python env
```bash
python3 -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
```

### 3) Add documents
```bash
mkdir -p documents
# Put your PDFs, Word, Markdown, HTML, CSV, TXTâ€¦ here
```

### 4) Run the app
```bash
streamlit run app.py
# Open http://localhost:8501
```

### 5) Use it
- Sidebar â†’ "ğŸ”„ Index New Documents" to index files
- Ask: â€œå¹«æˆ‘ç¸½çµé€™ä»½åˆç´„é‡é»ï¼Ÿâ€ / â€œExplain the API auth section.â€
- See sources and tweak search settings as needed

## ğŸ§‘â€ğŸ”¬ For Technical Users

The sections below explain how the app is built and how to tune it. If youâ€™re not technical, you can skip to Quick Start and be fine.

## ğŸ—ï¸ Architecture & Techniques

This app is a local, privacyâ€‘first RAG (Retrievalâ€‘Augmented Generation) system optimized for Apple Silicon (Mâ€‘series) Macs.

- Hybrid Retrieval: BM25 (Tantivy) + Vector (LanceDB) fused via Reciprocal Rank Fusion (RRF).
- Embeddings: Local Ollama model (default `embeddinggemma:300m`).
- Generation: Local Ollama model (default `gpt-oss:20b`).
- Reranking (optional): Crossâ€‘encoder models via `sentence-transformers` for higher relevance.
- Chunking: Characterâ€‘based by default; tokenâ€‘precise chunking via Tiktoken + spaCy available.
- Vision: PDF text + images + tables via PyMuPDF, pdfplumber; image understanding via LLaVA (pull `llava:7b`).
- Indexing: Incremental (xxHash change detection), content + similarity dedup (Faiss), rich metadata.
- Caching: Utilities for diskâ€‘based caches (diskcache) for embeddings and search results are included.
- Async/Batch: Async/batch modules using asyncio + executors are included.

Why itâ€™s fast and local:
- Tantivy (Rust) for BM25; LanceDB + PyArrow for vectors and metadata.
- ARM64â€‘friendly libs (xxHash, Faissâ€‘CPU, Tiktoken) tuned for Mâ€‘series.

## ğŸ§­ Project Structure

```
app.py                  # Streamlit entrypoint
src/
  core/                 # Retrieval, indexing, chat, caching, hybrid
    database.py         # LanceDB + PyArrow schema and access
    indexer.py          # Readers â†’ chunkers â†’ embeddings â†’ DB (+hybrid)
    hybrid_search.py    # Tantivy BM25 + Vector + RRF fusion
    embeddings.py       # Ollama embeddings
    search.py           # Search service (hybrid/vector, optional rerank)
    reranker.py         # Crossâ€‘encoder reranking (sentenceâ€‘transformers)
    query_expansion.py  # Synonyms/abbreviations/variations for recall
    cache.py            # Embedding + search result caching (diskcache)
    async_processor.py  # Async read/batch embedding/parallel search
    token_chunker.py    # Tokenâ€‘aware + sentence/codeâ€‘aware chunking
    deduplication.py    # xxHash content + Faiss similarity dedup
    chat.py             # LLM chat streaming with metrics
    vision_chat.py      # LLaVAâ€‘assisted visual Q&A for PDFs/images
  document_processing/
    readers.py          # PDF/DOCX/MD/HTML/CSV/XLSX/JSON readers
    vision_readers.py   # PDF images/tables via PyMuPDF + pdfplumber
    analyzer.py         # LLM doc type + language detection
    chunker.py          # Character + codeâ€‘aware chunking helpers
  config/
    settings.py         # Settings + .env loading
  ui/
    sidebar.py          # Indexing + search settings + language
    chat_interface.py   # Chat UI with metrics + citations
    settings_panel.py   # Presets, prompt path, Ollama checks
    modern_chat_history.py  # Enhanced chat history UI
    context_ring_widget.py  # Context visualization components
    i18n.py             # Localization utilities
  utils/
    logging_config.py   # Logging setup
    ollama_utils.py     # Ollama status/model checks
documents/              # Drop your source files here
.lancedb/               # Local vector store (autoâ€‘created)
tests (topâ€‘level)       # test_*.py quick scripts
```

## ğŸ”¬ How It Works

1) Ingest: Files in `documents/` are scanned recursively; readers normalize content.
2) Analyze: LLM detects doc type (meeting/prd/technical/wiki/general) and language.
3) Chunk: Characterâ€‘based or tokenâ€‘precise (Tiktoken + spaCy), with overlap and codeâ€‘aware handling.
4) Dedup: Exact (xxHash) + nearâ€‘duplicate (Faiss) pruning reduces noise and storage.
5) Embed: Local embeddings via Ollama; vectors + rich metadata stored in LanceDB.
6) Hybrid Search: Tantivy BM25 and vector results combined by RRF; optional crossâ€‘encoder reranking.
7) Generate: Context windows prepared with sources; local LLM answers with citations.
8) Vision: For PDFs, images/tables are extracted; LLaVA can be used to reason about visuals (requires an Ollama LLaVA model such as `llava:7b`).

Key metadata stored: `content_hash`, `doc_type`, `language`, `chunk_index`, `total_chunks`, `page_number`, `section_header`, `file_modified`.

## ğŸ§ª Techniques In Detail

- Hybrid Retrieval: Tantivy BM25 keyword matching + LanceDB cosine similarity; RRF balances both signals (adjust via â€œVector vs Keyword Weightâ€).
- Query Expansion: Synonyms/abbreviations/variations to improve recall on technical jargon.
- Reranking: Crossâ€‘encoders (fast/balanced/accurate) reâ€‘score top candidates for higher precision.
- Tokenâ€‘Aware Chunking: Tiktoken counts ensure safe context windows; sentence/code awareness improves coherence.
- Incremental Indexing: xxHash + mtime detect changes; old chunks are removed before reindexing.
- Deduplication: xxHash exact match + Faiss similarity deduplication.
- Caching: diskcache utilities for embeddings and search results (available in code and tests; not enabled by default in the main UI).
- Async/Batch: Parallel I/O and batched embeddings modules available (not enabled by default in the main UI).

## ğŸ¯ Who Itâ€™s For

- Developers and data/AI enthusiasts who want a local, noâ€‘cloud RAG.
- Product/ops teams that need quick Q&A over PRDs, specs, wikis, tickets.
- Researchers and students working with PDFs, notes, and mixedâ€‘format corpora.
- Privacyâ€‘sensitive users (legal, finance, healthcare) who cannot upload data.

## âœ… Best Use Cases

- Technical docs and codeâ€‘adjacent content where tokenâ€‘aware chunking matters.
- Product specs/PRDs and wikis where BM25 keyword hits complement semantic recall.
- Meeting notes where quick summarization and action extraction are useful.
- PDFâ€‘heavy material (charts/figures/tables) benefiting from vision support.

## ğŸ“Œ Example Scenarios

- Onboarding knowledge base: ingest handbooks, wikis, and SOPs; ask howâ€‘to questions.
- Contract/Policy review: drop PDFs; ask for summaries, obligations, exceptions (local only).
- Research roundup: papers + blog posts; query for findings and compare sections.
- Incident retros: aggregate logs/markdown notes; search hybrids improve recall.

## ğŸ”§ Configuration (Advanced)

Alongside the basic `.env` shown above, you can tune advanced behavior:

```env
# Search
HYBRID_SEARCH_ENABLED=true
DEFAULT_SEARCH_MODE=hybrid   # hybrid | vector | keyword
HYBRID_ALPHA=0.5             # 0=keywordâ€‘only, 1=vectorâ€‘only
SEARCH_RESULT_LIMIT=5

# Chunking (tokenâ€‘aware)
USE_TOKEN_CHUNKING=false
MAX_CHUNK_TOKENS=512
CHUNK_OVERLAP_TOKENS=50

# Reranking
USE_RERANKING=false
RERANKER_MODEL=balanced      # fast | balanced | accurate | multilingual
RERANK_TOP_K=5

# Deduplication
DEDUP_ENABLED=true
DEDUP_THRESHOLD=0.95

# UI / Locale / Prompt
DEFAULT_LOCALE=en
SYSTEM_PROMPT_PATH=rag_prompt.md

# Ollama
OLLAMA_HOST=http://localhost:11434
```

## ğŸ§‘â€ğŸ’» Run & Develop

- Setup env: `python3 -m venv .venv && source .venv/bin/activate && pip install -r requirements.txt`
- Run app: `streamlit run app.py` â†’ open `http://localhost:8501`
- Index: Sidebar â†’ â€œğŸ”„ Index New Documentsâ€
- Tests:
  - Core: `python test_indexing.py`, `python test_hybrid_search.py`
  - Phase 2: `python test_phase2_simple.py`, `python test_phase2_features.py`
  - Vision: `python test_vision.py` (ensure a PDF exists in `documents/`)

## âš–ï¸ Where This Shines

- 100% local, private by design; zero API keys needed.
- Strong on mixed corpora: PRDs/specs + wikis + PDFs with images.
- Tunable retrieval: slider to balance keyword vs vector; optional reranking for precision.
- Multilingual UX: English, ç¹é«”ä¸­æ–‡, ç®€ä½“ä¸­æ–‡, EspaÃ±ol, æ—¥æœ¬èª.

## âš ï¸ When It May Not Fit

- Very large, multiâ€‘tenant deployments needing distributed indices and auth.
- Teams requiring hosted SaaS or crossâ€‘device sync out of the box.
- Extremely latencyâ€‘sensitive workloads without local model warmâ€‘up.

## ğŸ§° Troubleshooting

- Ollama not detected: run `ollama serve` and pull models (`ollama pull embeddinggemma:300m` and `ollama pull gpt-oss:20b`). Use Settings â†’ â€œCheck Ollama Statusâ€.
- LanceDB corruption: the app autoâ€‘resets when detected; you can also click â€œğŸ§¹ Reset Indexâ€ in the sidebar.
- No results: make sure documents are indexed; increase Topâ€‘K; try Hybrid mode with `alphaâ‰ˆ0.5`.
- Slow first answer: first token is slower on cold models; improves after warmâ€‘up.
- Vision Q&A: works best with PDFs containing images/tables; limit very large PDFs.
  - If you see vision model errors, make sure youâ€™ve pulled a vision model: `ollama pull llava:7b` (or set `VISION_MODEL`).

## ğŸ§ª Tuning Tips

- Tech docs: Hybrid with alpha 0.5â€“0.7; Topâ€‘K 5â€“7; overlap 300â€“400.
- Meeting notes: Keywordâ€‘leaning (alpha 0.3â€“0.5); Topâ€‘K 3â€“5.
- PRDs/Wikis: Balanced hybrid (alpha 0.5); chunk 1500â€“1800.
- Token chunking: Enable for strict context windows or codeâ€‘heavy sets.

â€”

## ğŸ’¡ What It Does

- **Multiâ€‘file made easy**: Drop many files in `documents/`, index once, ask anything.
- **Ask your documents**: Chat in natural language and get answers with sources.
- **Read & Learn**: Summaries, explanations, and stepâ€‘byâ€‘step guidance from your files.
- **Search that understands**: Hybrid (keyword + vector) search for better results.
- **Local & free**: Runs entirely on your Mac with Ollama â€” no API keys.
 - **Vision support**: Understands PDFs with images, charts, and screenshots (via LLaVA; install a model like `llava:7b`).

<a id="quick-start-zh-hant"></a>
## å¿«é€Ÿé–‹å§‹ï¼ˆç¹ä¸­ï¼‰
æŠŠæª”æ¡ˆæ”¾é€² `documents/`ï¼Œç´¢å¼•å¾Œå°±èƒ½ç”¨ä¸­æ–‡/è‹±æ–‡æå•ã€‚

1) å®‰è£ï¼ˆmacOSï¼‰
- `brew install ollama`
- `ollama serve &`
- `ollama pull gpt-oss:20b`
- `ollama pull embeddinggemma:300m`
- `ollama pull llava:7b`  # ï¼ˆé¸ç”¨ï¼‰å½±åƒç†è§£æ¨¡å‹

2) å»ºç½®ç’°å¢ƒ
- `python3 -m venv .venv && source .venv/bin/activate`
- `pip install -r requirements.txt`

3) æ”¾å…¥æ–‡ä»¶ï¼šå»ºç«‹ `documents/` ä¸¦æ”¾å…¥ PDFã€Wordã€Markdownã€TXTâ€¦

4) åŸ·è¡Œï¼š`streamlit run app.py`ï¼ˆé–‹å•Ÿ http://localhost:8501ï¼‰

5) ä½¿ç”¨æ–¹å¼
- å´é‚Šæ¬„é»ã€ŒğŸ”„ å»ºç«‹æ–°ç´¢å¼•ã€ï¼Œç„¶å¾Œåœ¨å°è©±æ¡†æå•
- ä¾†æºèˆ‡æœå°‹è¨­å®šå¯åœ¨é é¢ä¸­æŸ¥çœ‹èˆ‡èª¿æ•´

<a id="quick-start-zh-hans"></a>
## å¿«é€Ÿå¼€å§‹ï¼ˆç®€ä¸­ï¼‰
æŠŠæ–‡ä»¶æ”¾è¿› `documents/`ï¼Œå»ºç«‹ç´¢å¼•åå³å¯ä¸­/è‹±æ–‡æé—®ã€‚

1) å®‰è£…ï¼ˆmacOSï¼‰
- `brew install ollama`
- `ollama serve &`
- `ollama pull gpt-oss:20b`
- `ollama pull embeddinggemma:300m`
- `ollama pull llava:7b`  # ï¼ˆå¯é€‰ï¼‰å›¾åƒç†è§£æ¨¡å‹

2) ç¯å¢ƒ
- `python3 -m venv .venv && source .venv/bin/activate`
- `pip install -r requirements.txt`

3) æ”¾å…¥æ–‡ä»¶ï¼šåˆ›å»º `documents/`ï¼ŒåŠ å…¥ PDFã€Wordã€Markdownã€TXTâ€¦

4) è¿è¡Œï¼š`streamlit run app.py`ï¼ˆæ‰“å¼€ http://localhost:8501ï¼‰

5) ç”¨æ³•
- ä¾§è¾¹æ ç‚¹â€œğŸ”„ å»ºç«‹æ–°ç´¢å¼•â€ï¼Œç„¶ååœ¨å¯¹è¯æ¡†æé—®
- å¯æŸ¥çœ‹æ¥æºå¹¶è°ƒæ•´æœç´¢è®¾ç½®

<a id="quick-start-es"></a>
## Inicio RÃ¡pido (ES)
Coloca archivos en `documents/`, indexa y pregunta en espaÃ±ol o inglÃ©s.

1) Instala (macOS)
- `brew install ollama`
- `ollama serve &`
- `ollama pull gpt-oss:20b`
- `ollama pull embeddinggemma:300m`
- `ollama pull llava:7b`  # (Opcional) modelo de visiÃ³n

2) Entorno
- `python3 -m venv .venv && source .venv/bin/activate`
- `pip install -r requirements.txt`

3) AÃ±ade documentos: crea `documents/` y pon PDF, Word, Markdown, TXTâ€¦

4) Ejecuta: `streamlit run app.py` (abre http://localhost:8501)

5) Uso
- Barra lateral â†’ â€œğŸ”„ Indexar nuevos documentosâ€ y luego pregunta en el chat
- Revisa fuentes y ajusta la bÃºsqueda

<a id="quick-start-ja"></a>
## ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ (JA)
`documents/` ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å…¥ã‚Œã¦ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã€ãã®å¾Œãƒãƒ£ãƒƒãƒˆã§è³ªå•ã€‚

1) ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆmacOSï¼‰
- `brew install ollama`
- `ollama serve &`
- `ollama pull gpt-oss:20b`
- `ollama pull embeddinggemma:300m`
- `ollama pull llava:7b`  # ï¼ˆä»»æ„ï¼‰ç”»åƒç†è§£ãƒ¢ãƒ‡ãƒ«

2) ç’°å¢ƒ
- `python3 -m venv .venv && source .venv/bin/activate`
- `pip install -r requirements.txt`

3) ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè¿½åŠ ï¼š`documents/` ã‚’ä½œæˆã— PDF / Word / Markdown / TXTâ€¦ ã‚’é…ç½®

4) å®Ÿè¡Œï¼š`streamlit run app.py`ï¼ˆhttp://localhost:8501 ã‚’é–‹ãï¼‰

5) ä½¿ã„æ–¹
- ã‚µã‚¤ãƒ‰ãƒãƒ¼ã€ŒğŸ”„ æ–°ã—ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã€ã§ç™»éŒ² â†’ ãƒãƒ£ãƒƒãƒˆã§è³ªå•
- å‡ºå…¸ã®ç¢ºèªã‚„æ¤œç´¢è¨­å®šã®èª¿æ•´ãŒå¯èƒ½

## âœ¨ Features

- **Formats**: PDF, DOCX/DOC, MD, HTML, CSV, XLSX/XLS, JSON, TXT, LOG, RTF, XML, YAML
- **Presets**: Meeting notes, PRD/specs, tech docs, wiki/KB
- **Controls**: Chunk size, overlap, results count, hybrid weight
- **Metrics**: Response time, first token, tokens/sec
- **Index management**: Reset/clear index when needed

## âš™ï¸ Configuration (.env optional)
```env
DB_DIR=.lancedb
TABLE_NAME=docs
EMBED_MODEL=embeddinggemma:300m
GEN_MODEL=gpt-oss:20b
DOCUMENTS_FOLDER=documents
VISION_MODEL=llava:7b
```

## Tips

- Notion/é£›æ›¸(Feishu/Lark) è«‹åŒ¯å‡ºç‚º Markdown å†æ”¾é€² `documents/`ã€‚
- æŠ€è¡“æ–‡ä»¶å¯æŠŠ chunk è¨­å¤§ä¸€äº›ï¼ˆ1500â€“1800ï¼‰ï¼ŒOverlap 300â€“400ã€‚
- æ‰¾ä¸åˆ°ç­”æ¡ˆæ™‚ï¼šå¤šæ”¾å¹¾ä»½æª”æ¡ˆæˆ–æé«˜ Topâ€‘Kã€‚

## Languages

- Supported: English, ç¹é«”ä¸­æ–‡ (zhâ€‘Hant), ç®€ä½“ä¸­æ–‡ (zhâ€‘Hans), EspaÃ±ol (es), æ—¥æœ¬èª (ja)
- First load autoâ€‘detects from browser language; ambiguous `zh` defaults to ç¹é«” (zhâ€‘Hant)
- Change anytime via sidebar selector: â€œğŸŒ Language / èªè¨€ / è¯­è¨€â€
- Optional default in `.env`: `DEFAULT_LOCALE=zh-Hant` (or `en`, `zh-Hans`, `es`, `ja`)
- URL override: add `?locale=ja` (or `en`, `zh-Hant`, `zh-Hans`, `es`)

## Testing (optional)
- Core tests: `python test_indexing.py`, `python test_hybrid_search.py`
- Phase 2: `python test_phase2_simple.py`, `python test_phase2_features.py`
- Vision: `python test_vision.py`ï¼ˆéœ€åœ¨ `documents/` æ”¾ä¸€ä»½ PDFï¼‰

## Privacy & Requirements
- 100% æœ¬åœ°é‹è¡Œï¼Œè³‡æ–™ä¸å‡ºæ©Ÿå™¨ï¼›ç„¡éœ€é›²ç«¯ API é‡‘é‘°ã€‚
- å»ºè­° 16GB+ RAMï¼›æ¨¡å‹ä¸‹è¼‰ç´„éœ€ 10â€“15GB ç©ºé–“ã€‚

â€”

Made with love for everyone who wants AIâ€‘powered document help without paying for the cloud.
